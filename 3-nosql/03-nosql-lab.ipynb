{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Relational to NoSQL to Graph: Building a Data Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b747b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from pymongo import MongoClient\n",
    "import json, glob, os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Database (SQLite)\n",
    "\n",
    "We start with a traditional metadata catalog â€” a relational schema describing datasets and their columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- agent ---\n",
      "[(1, 'alice'), (2, 'bob'), (3, 'carol')]\n",
      "\n",
      "--- entity ---\n",
      "[(1, 'a', 'xml', '2025-02-03T00:00:00', None), (2, 'b', 'tiff', '2026-05-10T00:00:00', None), (3, 'c', 'json', '2026-04-19T00:00:00', None), (4, 'd', 'xlsx', '2025-10-12T00:00:00', None), (5, 'e', 'json', '2025-02-05T00:00:00', None), (6, 'f', 'parquet', '2025-03-19T00:00:00', 3), (7, 'g', 'csv', '2026-03-07T00:00:00', None), (8, 'h', 'xml', '2025-12-30T00:00:00', 3), (9, 'i', 'xlsx', '2026-11-23T00:00:00', 3), (10, 'j', 'avro', '2025-11-07T00:00:00', None)]\n",
      "\n",
      "--- wasAttributedTo ---\n",
      "[(1, 1), (2, 2), (3, 1), (4, 2), (5, 3), (6, 2), (7, 1), (8, 3), (9, 1), (10, 1)]\n",
      "\n",
      "--- activity ---\n",
      "[(1, 'load_inventory_snapshot'), (2, 'transform_customer_data'), (3, 'convert_image_formats'), (4, 'clean_web_logs'), (5, 'validate_financial_forecast')]\n",
      "\n",
      "--- wasGeneratedBy ---\n",
      "[(1, 4), (2, 6), (3, 8), (4, 7), (5, 8), (6, 2), (7, 6), (8, 3), (9, 4), (10, 7)]\n"
     ]
    }
   ],
   "source": [
    "# --- CREATE IN-MEMORY DATABASE ---\n",
    "# Using \":memory:\" creates a temporary DB that exists only while the script runs\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# --- CREATE TABLE: agent ---\n",
    "# Represents people or systems responsible for data entities\n",
    "cur.execute('''\n",
    "CREATE TABLE agent (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert a few example agents\n",
    "agents = [\"alice\", \"bob\", \"carol\"]\n",
    "cur.executemany(\"INSERT INTO agent(name) VALUES (?)\", [(agent,) for agent in agents])\n",
    "\n",
    "# --- CREATE TABLE: entity ---\n",
    "# Represents datasets or data files, each with a name, type, and creation timestamp\n",
    "# The wasDerivedFrom field models lineage (self-reference to another entity)\n",
    "cur.execute('''\n",
    "CREATE TABLE entity (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT,\n",
    "    file_type TEXT,\n",
    "    created_at TEXT,\n",
    "    wasDerivedFrom REFERENCES entity(id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Define possible file types and arbitrary dataset names (topics)\n",
    "file_types = [\"csv\", \"json\", \"parquet\", \"tiff\", \"xml\", \"avro\", \"xlsx\"]\n",
    "topics = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"]\n",
    "\n",
    "# Generate synthetic entities with random properties\n",
    "entities = []\n",
    "base_date = datetime(2025, 1, 1)\n",
    "\n",
    "for name in topics:\n",
    "    file_type = random.choice(file_types)\n",
    "    created_at = (base_date + timedelta(days=random.randint(0, 700))).isoformat()\n",
    "    # Randomly link some entities to a previous one to simulate data derivation\n",
    "    was_derived_from = random.randint(1, len(entities)) if entities and random.random() < 0.5 else None\n",
    "    entities.append((name, file_type, created_at, was_derived_from))\n",
    "\n",
    "# Insert generated entities into the database\n",
    "cur.executemany(\n",
    "    \"INSERT INTO entity(name, file_type, created_at, wasDerivedFrom) VALUES (?, ?, ?, ?)\",\n",
    "    entities\n",
    ")\n",
    "conn.commit()\n",
    "\n",
    "# --- CREATE TABLE: wasAttributedTo ---\n",
    "# Connects entities to the agents responsible for them\n",
    "cur.execute('''\n",
    "CREATE TABLE wasAttributedTo (\n",
    "    entity_id REFERENCES entity(id),\n",
    "    agent_id REFERENCES agent(id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Randomly assign each entity to one of the agents\n",
    "relations = []\n",
    "for e_id in range(len(entities)):\n",
    "    relations.append([e_id + 1, random.choice(range(len(agents))) + 1])\n",
    "\n",
    "cur.executemany(\"INSERT INTO wasAttributedTo(entity_id, agent_id) VALUES (?, ?)\", relations)\n",
    "conn.commit()\n",
    "\n",
    "# --- CREATE TABLE: activity ---\n",
    "# Represents data processing steps (ETL-style)\n",
    "cur.execute('''\n",
    "CREATE TABLE activity (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# List of possible ETL activities\n",
    "activities = [\n",
    "    \"extract_sales_data\",\n",
    "    \"transform_customer_data\",\n",
    "    \"load_inventory_snapshot\",\n",
    "    \"aggregate_marketing_metrics\",\n",
    "    \"clean_web_logs\",\n",
    "    \"validate_financial_forecast\",\n",
    "    \"normalize_sensor_data\",\n",
    "    \"convert_image_formats\"\n",
    "]\n",
    "\n",
    "# Randomly insert 5 activities into the table\n",
    "cur.executemany(\"INSERT INTO activity(name) VALUES (?)\", [(a,) for a in random.sample(activities, k=5)])\n",
    "conn.commit()\n",
    "\n",
    "# --- CREATE TABLE: wasGeneratedBy ---\n",
    "# Links entities to the activities that produced them\n",
    "cur.execute('''\n",
    "CREATE TABLE wasGeneratedBy (\n",
    "    entity_id REFERENCES entity(id),\n",
    "    activity_id REFERENCES activity(id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Randomly associate each entity with one activity\n",
    "relations = []\n",
    "for e_id in range(len(entities)):\n",
    "    relations.append([e_id + 1, random.choice(range(len(activities))) + 1])\n",
    "\n",
    "cur.executemany(\"INSERT INTO wasGeneratedBy(entity_id, activity_id) VALUES (?, ?)\", relations)\n",
    "conn.commit()\n",
    "\n",
    "# --- DISPLAY TABLE CONTENTS ---\n",
    "# Print all data for quick inspection and verification\n",
    "for table in [\"agent\", \"entity\", \"wasAttributedTo\", \"activity\", \"wasGeneratedBy\"]:\n",
    "    print(f\"\\n--- {table} ---\")\n",
    "    cur.execute(f\"SELECT * FROM {table}\")\n",
    "    print([row for row in cur.fetchall()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Database (MongoDB)\n",
    "\n",
    "Now we move to MongoDB to handle flexible, nested, and heterogeneous metadata structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'a', 'file_type': 'xml', 'created_at': '2025-02-03T00:00:00', 'agents': ['alice'], 'activities': ['clean_web_logs'], 'wasDerivedFrom': None, 'notes': 'generic metadata'}\n",
      "{'name': 'b', 'file_type': 'tiff', 'created_at': '2026-05-10T00:00:00', 'agents': ['bob'], 'activities': [], 'wasDerivedFrom': None, 'resolution': '4k', 'color_depth': 24}\n",
      "{'name': 'c', 'file_type': 'json', 'created_at': '2026-04-19T00:00:00', 'agents': ['alice'], 'activities': [], 'wasDerivedFrom': None, 'schema_version': 'v1', 'nested': True}\n",
      "{'name': 'd', 'file_type': 'xlsx', 'created_at': '2025-10-12T00:00:00', 'agents': ['bob'], 'activities': [], 'wasDerivedFrom': None, 'sheet_count': 3, 'has_formulas': False}\n",
      "{'name': 'e', 'file_type': 'json', 'created_at': '2025-02-05T00:00:00', 'agents': ['carol'], 'activities': [], 'wasDerivedFrom': None, 'schema_version': 'v5', 'nested': False}\n",
      "{'name': 'f', 'file_type': 'parquet', 'created_at': '2025-03-19T00:00:00', 'agents': ['bob'], 'activities': ['transform_customer_data'], 'wasDerivedFrom': 3, 'notes': 'generic metadata'}\n",
      "{'name': 'g', 'file_type': 'csv', 'created_at': '2026-03-07T00:00:00', 'agents': ['alice'], 'activities': [], 'wasDerivedFrom': None, 'columns': 12, 'delimiter': ',', 'encoding': 'utf-8'}\n",
      "{'name': 'h', 'file_type': 'xml', 'created_at': '2025-12-30T00:00:00', 'agents': ['carol'], 'activities': ['convert_image_formats'], 'wasDerivedFrom': 3, 'notes': 'generic metadata'}\n",
      "{'name': 'i', 'file_type': 'xlsx', 'created_at': '2026-11-23T00:00:00', 'agents': ['alice'], 'activities': ['clean_web_logs'], 'wasDerivedFrom': 3, 'sheet_count': 3, 'has_formulas': True}\n",
      "{'name': 'j', 'file_type': 'avro', 'created_at': '2025-11-07T00:00:00', 'agents': ['alice'], 'activities': [], 'wasDerivedFrom': None, 'compression': 'snappy', 'record_count': 58874}\n"
     ]
    }
   ],
   "source": [
    "# Load .env from parent folder\n",
    "load_dotenv(dotenv_path=os.path.join(\"..\", \".env\"))\n",
    "\n",
    "mongo_uri = f\"mongodb://{os.getenv('IP')}:27017/db\"\n",
    "if not mongo_uri:\n",
    "    raise ValueError(\"MONGO_URI not set in .env\")\n",
    "\n",
    "# --- CONNECT TO MONGODB ---\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[\"metadata_catalog\"]\n",
    "\n",
    "# Clean existing collections\n",
    "for col in [\"agents\", \"activities\", \"entities\"]:\n",
    "    db[col].delete_many({})\n",
    "\n",
    "# --- MIGRATE AGENTS ---\n",
    "cur.execute(\"SELECT id, name FROM agent\")\n",
    "agents = [{\"_id\": a_id, \"name\": name} for a_id, name in cur.fetchall()]\n",
    "db.agents.insert_many(agents)\n",
    "\n",
    "# --- MIGRATE ACTIVITIES ---\n",
    "cur.execute(\"SELECT id, name FROM activity\")\n",
    "activities = [{\"_id\": a_id, \"name\": name} for a_id, name in cur.fetchall()]\n",
    "db.activities.insert_many(activities)\n",
    "\n",
    "# --- GET RELATIONS ---\n",
    "cur.execute(\"SELECT entity_id, agent_id FROM wasAttributedTo\")\n",
    "entity_to_agents = {}\n",
    "for e_id, a_id in cur.fetchall():\n",
    "    entity_to_agents.setdefault(e_id, []).append(a_id)\n",
    "\n",
    "cur.execute(\"SELECT entity_id, activity_id FROM wasGeneratedBy\")\n",
    "entity_to_activities = {}\n",
    "for e_id, act_id in cur.fetchall():\n",
    "    entity_to_activities.setdefault(e_id, []).append(act_id)\n",
    "\n",
    "# --- FUNCTION TO CREATE CUSTOM METADATA ---\n",
    "def generate_metadata(file_type):\n",
    "    if file_type == \"csv\":\n",
    "        return {\"columns\": random.randint(3, 12), \"delimiter\": \",\", \"encoding\": \"utf-8\"}\n",
    "    elif file_type == \"json\":\n",
    "        return {\"schema_version\": f\"v{random.randint(1, 5)}\", \"nested\": random.choice([True, False])}\n",
    "    elif file_type == \"tiff\":\n",
    "        return {\"resolution\": random.choice([\"1080p\", \"4k\"]), \"color_depth\": random.choice([8, 16, 24])}\n",
    "    elif file_type == \"xlsx\":\n",
    "        return {\"sheet_count\": random.randint(1, 10), \"has_formulas\": random.choice([True, False])}\n",
    "    elif file_type == \"avro\":\n",
    "        return {\"compression\": random.choice([\"snappy\", \"deflate\"]), \"record_count\": random.randint(1000, 100000)}\n",
    "    else:\n",
    "        return {\"notes\": \"generic metadata\"}\n",
    "\n",
    "# --- MIGRATE ENTITIES ---\n",
    "cur.execute(\"SELECT id, name, file_type, created_at, wasDerivedFrom FROM entity\")\n",
    "mongo_entities = []\n",
    "\n",
    "for e_id, name, file_type, created_at, wasDerivedFrom in cur.fetchall():\n",
    "    # get related agents and activities by id\n",
    "    related_agent_ids = entity_to_agents.get(e_id, [])\n",
    "    related_activity_ids = entity_to_activities.get(e_id, [])\n",
    "\n",
    "    # resolve names\n",
    "    cur.execute(\"SELECT name FROM agent WHERE id IN (%s)\" % \",\".join(\"?\" * len(related_agent_ids)), related_agent_ids or [])\n",
    "    agents = [row[0] for row in cur.fetchall()] if related_agent_ids else []\n",
    "\n",
    "    cur.execute(\"SELECT name FROM activity WHERE id IN (%s)\" % \",\".join(\"?\" * len(related_activity_ids)), related_activity_ids or [])\n",
    "    activities = [row[0] for row in cur.fetchall()] if related_activity_ids else []\n",
    "\n",
    "    # flatten metadata\n",
    "    metadata = generate_metadata(file_type)\n",
    "\n",
    "    # base document\n",
    "    doc = {\n",
    "        \"_id\": e_id,\n",
    "        \"name\": name,\n",
    "        \"file_type\": file_type,\n",
    "        \"created_at\": created_at,\n",
    "        \"agents\": agents,\n",
    "        \"activities\": activities,\n",
    "        \"wasDerivedFrom\": wasDerivedFrom\n",
    "    }\n",
    "\n",
    "    # merge metadata as top-level fields\n",
    "    doc.update(metadata)\n",
    "\n",
    "    mongo_entities.append(doc)\n",
    "\n",
    "db.entities.insert_many(mongo_entities)\n",
    "\n",
    "for e in db.entities.find({}, {\"_id\": 0}):\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Database (Neo4j)\n",
    "\n",
    "Now we model lineage and dependencies between datasets, jobs, and owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent']: {'name': 'alice', 'id': 1}\n",
      "['Agent']: {'name': 'bob', 'id': 2}\n",
      "['Agent']: {'name': 'carol', 'id': 3}\n",
      "['Activity']: {'name': 'load_inventory_snapshot', 'id': 1}\n",
      "['Activity']: {'name': 'transform_customer_data', 'id': 2}\n",
      "['Activity']: {'name': 'convert_image_formats', 'id': 3}\n",
      "['Activity']: {'name': 'clean_web_logs', 'id': 4}\n",
      "['Activity']: {'name': 'validate_financial_forecast', 'id': 5}\n",
      "['Entity']: {'notes': 'generic metadata', 'file_type': 'xml', 'name': 'a', 'created_at': '2025-02-03T00:00:00', 'id': 1}\n",
      "['Entity']: {'file_type': 'tiff', 'name': 'b', 'created_at': '2026-05-10T00:00:00', 'color_depth': 24, 'id': 2, 'resolution': '4k'}\n",
      "['Entity']: {'schema_version': 'v1', 'file_type': 'json', 'name': 'c', 'created_at': '2026-04-19T00:00:00', 'id': 3, 'nested': True}\n",
      "['Entity']: {'sheet_count': 3, 'has_formulas': False, 'file_type': 'xlsx', 'name': 'd', 'created_at': '2025-10-12T00:00:00', 'id': 4}\n",
      "['Entity']: {'schema_version': 'v5', 'file_type': 'json', 'name': 'e', 'created_at': '2025-02-05T00:00:00', 'id': 5, 'nested': False}\n",
      "['Entity']: {'notes': 'generic metadata', 'file_type': 'parquet', 'name': 'f', 'created_at': '2025-03-19T00:00:00', 'id': 6}\n",
      "['Entity']: {'delimiter': ',', 'file_type': 'csv', 'columns': 12, 'name': 'g', 'created_at': '2026-03-07T00:00:00', 'id': 7, 'encoding': 'utf-8'}\n",
      "['Entity']: {'notes': 'generic metadata', 'file_type': 'xml', 'name': 'h', 'created_at': '2025-12-30T00:00:00', 'id': 8}\n",
      "['Entity']: {'sheet_count': 3, 'has_formulas': True, 'file_type': 'xlsx', 'name': 'i', 'created_at': '2026-11-23T00:00:00', 'id': 9}\n",
      "['Entity']: {'record_count': 58874, 'file_type': 'avro', 'name': 'j', 'created_at': '2025-11-07T00:00:00', 'id': 10, 'compression': 'snappy'}\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=os.path.join(\"..\", \".env\"))\n",
    "\n",
    "uri = f\"bolt://{os.getenv('IP')}:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"password\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "with driver.session() as session:\n",
    "    # Clear Neo4j\n",
    "    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "\n",
    "    # --- Create Agent nodes ---\n",
    "    for agent in db.agents.find():\n",
    "        session.run(\n",
    "            \"CREATE (:Agent {id: $id, name: $name})\",\n",
    "            id=agent[\"_id\"],\n",
    "            name=agent[\"name\"]\n",
    "        )\n",
    "\n",
    "    # --- Create Activity nodes ---\n",
    "    for act in db.activities.find():\n",
    "        session.run(\n",
    "            \"CREATE (:Activity {id: $id, name: $name})\",\n",
    "            id=act[\"_id\"],\n",
    "            name=act[\"name\"]\n",
    "        )\n",
    "\n",
    "    # --- Create Entity nodes ---\n",
    "    for e in db.entities.find():\n",
    "        # Flatten entity properties (skip agents, activities, wasDerivedFrom)\n",
    "        props = {}\n",
    "        for k, v in e.items():\n",
    "            if k in [\"_id\", \"agents\", \"activities\", \"wasDerivedFrom\"]:\n",
    "                continue\n",
    "            elif isinstance(v, (str, int, float, bool)):\n",
    "                props[k] = v\n",
    "            elif isinstance(v, list):\n",
    "                props[k] = [str(x) for x in v]\n",
    "            elif isinstance(v, dict):\n",
    "                # flatten dict metadata into key-value pairs\n",
    "                for mk, mv in v.items():\n",
    "                    props[mk] = str(mv)\n",
    "            elif v is None:\n",
    "                continue\n",
    "            else:\n",
    "                props[k] = str(v)\n",
    "\n",
    "        props[\"id\"] = e[\"_id\"]\n",
    "\n",
    "        # Build dynamic CREATE statement\n",
    "        prop_keys = \", \".join([f\"{k}: ${k}\" for k in props])\n",
    "        cypher = f\"CREATE (ent:Entity {{ {prop_keys} }})\"\n",
    "        session.run(cypher, **props)\n",
    "\n",
    "    # --- Create Relationships ---\n",
    "    for e in db.entities.find():\n",
    "        # Derived relationships\n",
    "        if e.get(\"wasDerivedFrom\") is not None:\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MATCH (src:Entity {id: $derived_id}), (dst:Entity {id: $entity_id})\n",
    "                CREATE (dst)-[:WAS_DERIVED_FROM]->(src)\n",
    "                \"\"\",\n",
    "                derived_id=e[\"wasDerivedFrom\"],\n",
    "                entity_id=e[\"_id\"]\n",
    "            )\n",
    "\n",
    "        # Agent relationships\n",
    "        for agent_name in e.get(\"agents\", []):\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MATCH (a:Agent {name: $agent_name}), (ent:Entity {id: $entity_id})\n",
    "                CREATE (a)-[:WAS_ATTRIBUTED_TO]->(ent)\n",
    "                \"\"\",\n",
    "                agent_name=agent_name,\n",
    "                entity_id=e[\"_id\"]\n",
    "            )\n",
    "\n",
    "        # Activity relationships\n",
    "        for act_name in e.get(\"activities\", []):\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MATCH (act:Activity {name: $act_name}), (ent:Entity {id: $entity_id})\n",
    "                CREATE (act)-[:WAS_GENERATED_BY]->(ent)\n",
    "                \"\"\",\n",
    "                act_name=act_name,\n",
    "                entity_id=e[\"_id\"]\n",
    "            )\n",
    "\n",
    "    # Fetch all nodes and their labels/properties\n",
    "    result = session.run(\"MATCH (n) RETURN labels(n) AS labels, n LIMIT 100\")\n",
    "    for record in result:\n",
    "        labels = record[\"labels\"]\n",
    "        node = record[\"n\"]\n",
    "        print(f\"{labels}: {dict(node)}\")\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
